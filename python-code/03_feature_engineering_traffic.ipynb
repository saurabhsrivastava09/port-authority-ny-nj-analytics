{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b553ac-7b27-40f1-9e51-103872d0f84e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading and cleaning chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:19<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Finished loading. Shape: (5383378, 30)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”¹ ENABLE TQDM PROGRESS BARS FOR PANDAS\n",
    "# ============================================================\n",
    "tqdm.pandas()  # adds progress_apply to pandas\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1ï¸) LOAD DATA (with progress tracking & efficient cleaning)\n",
    "# ============================================================\n",
    "\n",
    "# Initialize tqdm progress bar for chunks\n",
    "chunksize = 500_000\n",
    "filename = \"All Recorded Traffic.txt\"\n",
    "\n",
    "# Count total lines first (optional, but lets tqdm show % complete)\n",
    "with open(filename, 'r') as f:\n",
    "    total_lines = sum(1 for _ in f)\n",
    "total_chunks = total_lines // chunksize + 1\n",
    "\n",
    "parts = []\n",
    "\n",
    "# Read in chunks with progress bar\n",
    "for ch in tqdm(pd.read_csv(filename, sep=\"\\t\", dtype=str, chunksize=chunksize),\n",
    "               total=total_chunks, desc=\"Reading and cleaning chunks\"):\n",
    "    # Clean column names\n",
    "    ch.columns = ch.columns.str.strip()\n",
    "\n",
    "    # Strip whitespace only in string columns\n",
    "    str_cols = ch.select_dtypes(include=[\"object\"]).columns\n",
    "    ch[str_cols] = ch[str_cols].apply(lambda col: col.str.strip())\n",
    "\n",
    "    parts.append(ch)\n",
    "\n",
    "# Combine all chunks into one dataframe\n",
    "traffic = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "print(f\"\\n Finished loading. Shape: {traffic.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5318555d-1810-4171-bb8b-7c8d7501e010",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Normalizing TIME column...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5383378/5383378 [00:15<00:00, 341588.60it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5383378/5383378 [12:26<00:00, 7208.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2ï¸) NORMALIZE DATE / TIME\n",
    "# ============================================================\n",
    "\n",
    "traffic['DATE'] = pd.to_datetime(traffic['DATE'], errors='coerce')\n",
    "\n",
    "def normalize_time(t):\n",
    "    \"\"\"Standardize various time formats into consistent HH:MM:SS.\"\"\"\n",
    "    if pd.isna(t):\n",
    "        return pd.NaT\n",
    "    s = str(t).strip()\n",
    "    if ':' in s:\n",
    "        s = s.split('.')[0]\n",
    "        for fmt in ['%H:%M:%S', '%H:%M']:\n",
    "            try:\n",
    "                return pd.to_datetime(s, format=fmt).time()\n",
    "            except:\n",
    "                continue\n",
    "        return pd.NaT\n",
    "    digits = ''.join(ch for ch in s if ch.isdigit()).zfill(4)[-4:]\n",
    "    try:\n",
    "        return datetime.strptime(digits, '%H%M').time()\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "print(\"\\n Normalizing TIME column...\")\n",
    "traffic['TIME_parsed'] = traffic['TIME'].progress_apply(normalize_time)\n",
    "\n",
    "traffic['DATETIME'] = traffic.progress_apply(\n",
    "    lambda r: pd.NaT if pd.isna(r['DATE']) or pd.isna(r['TIME_parsed'])\n",
    "    else pd.to_datetime(f\"{r['DATE'].date()} {r['TIME_parsed'].strftime('%H:%M:%S')}\"),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d9929a8-faba-4634-8f12-54db58da6e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Converting numeric columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Numeric conversion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:11<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns converted\n",
      "\n",
      "Converting CLASS columns and filling NaNs with 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLASS numeric conversion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS columns converted and NaNs filled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3ï¸) NUMERIC CONVERSION\n",
    "# ============================================================\n",
    "\n",
    "num_cols = ['TOTAL', 'CASH', 'EZPASS', 'VIOLATION', 'Autos', 'Small_T', 'Large_T', 'Buses']\n",
    "\n",
    "print(\"\\n Converting numeric columns...\")\n",
    "for c in tqdm(num_cols, desc=\"Numeric conversion\"):\n",
    "    if c in traffic.columns:\n",
    "        traffic[c] = pd.to_numeric(traffic[c].replace(['NULL', ''], np.nan), errors='coerce')\n",
    "print(\"Numeric columns converted\")\n",
    "\n",
    "# --- Convert CLASS columns to numeric to avoid Parquet errors. And Fill nulls in CLASS columns with 0\n",
    "class_cols = [c for c in traffic.columns if c.upper().startswith('CLASS')]\n",
    "print(\"\\nConverting CLASS columns and filling NaNs with 0...\")\n",
    "for c in tqdm(class_cols, desc=\"CLASS numeric conversion\"):\n",
    "    traffic[c] = pd.to_numeric(traffic[c], errors='coerce').fillna(0).astype(float)\n",
    "print(\"CLASS columns converted and NaNs filled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1950e8a-a3b4-488e-a9d8-c8a274414afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Calculating class sums...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summing CLASS columns: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 61.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mismatch rows: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4ï¸) CLASS SUM CHECK (with progress)\n",
    "# ============================================================\n",
    "\n",
    "class_cols = [c for c in traffic.columns if c.upper().startswith('CLASS')]\n",
    "\n",
    "if class_cols:\n",
    "    print(\"\\nðŸ”„ Calculating class sums...\")\n",
    "    traffic['class_sum'] = 0\n",
    "    for c in tqdm(class_cols, desc=\"Summing CLASS columns\"):\n",
    "        traffic['class_sum'] += traffic[c].fillna(0).astype(float)\n",
    "    traffic['class_mismatch'] = (traffic['TOTAL'] - traffic['class_sum']).abs() > 0.01\n",
    "    mismatch_count = traffic['class_mismatch'].sum()\n",
    "    print(f\"Class mismatch rows: {mismatch_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "700d55fc-30a5-4af0-95de-a12a3904f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Drop redundant columns\n",
    "traffic = traffic.drop(columns=['Month', 'Yr', 'FAC_G2'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecfa293-c30a-425b-96b7-1a00053edc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5ï¸) STRATIFIED SAMPLING (5%) â€” progress optional\n",
    "# ============================================================\n",
    "\n",
    "#traffic['month_year'] = traffic['DATE'].dt.to_period('M')\n",
    "\n",
    "#print(\"\\n Creating stratified 5% sample (safe for all pandas versions)...\")\n",
    "#sample = (\n",
    "#    traffic.groupby(['FAC_B', 'month_year'], group_keys=False)\n",
    "#           .progress_apply(lambda x: x.sample(frac=0.05, random_state=1) if len(x) > 1 else x)\n",
    "#           .reset_index(drop=True)  # ensures no duplicate index issues\n",
    "#)\n",
    "#print(f\"Stratified sample created. Sample shape: {sample.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf99be3c-f01d-4a68-8fdc-99ef0b0e5515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Saving final datasets with progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:35<00:00, 17.95s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " All steps completed successfully.\n",
      "âž¡ Saved files:\n",
      "   - EDA_All_Recording_Traffic.parquet  (full cleaned dataset)\n",
      "   - EDA_All_Recording_Traffic.txt  (full dataset tab-delimited)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6ï¸) SAVE FINAL DATASETS\n",
    "# ============================================================\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time  # just for simulating progress\n",
    "\n",
    "# List of files to save with descriptions\n",
    "save_tasks = [\n",
    "    #('EDA_All_Recording_Traffic_Sample.csv', lambda: sample.to_csv('EDA_Ready_All_Recording_Traffic_Sample.csv', index=False), 'Sample dataset (5%)'),\n",
    "    ('EDA_All_Recording_Traffic.parquet', lambda: traffic.to_parquet('EDA_Ready_All_Recording_Traffic.parquet', index=False), 'Full cleaned dataset'),\n",
    "    ('EDA_All_Recording_Traffic.txt', lambda: traffic.to_csv('EDA_Ready_All_Recording_Traffic.txt', sep='\\t', index=False), 'Full dataset tab-delimited')\n",
    "]\n",
    "\n",
    "print(\"\\n Saving final datasets with progress:\")\n",
    "\n",
    "for filename, func, desc in tqdm(save_tasks, desc=\"Saving files\", unit=\"file\"):\n",
    "    func()  # execute the save function\n",
    "    # Optional: tiny sleep to visually show progress if fast\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(\"\\n All steps completed successfully.\")\n",
    "print(\"âž¡ Saved files:\")\n",
    "#print(\"   - EDA_All_Recording_Traffic_Sample.csv  (for quick analysis)\")\n",
    "print(\"   - EDA_All_Recording_Traffic.parquet  (full cleaned dataset)\")\n",
    "print(\"   - EDA_All_Recording_Traffic.txt  (full dataset tab-delimited)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46027f1-8a6b-4621-be7c-9883ec4c5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "985016a8-1a6d-47f5-86e7-e6ecda87205d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5383378/5383378 [00:01<00:00, 2742733.90it/s]\n",
      "Processing rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5383378/5383378 [00:16<00:00, 323950.17it/s]\n",
      "Processing rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5383378/5383378 [00:01<00:00, 3476634.44it/s]\n",
      "Processing rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5383378/5383378 [00:01<00:00, 3438012.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating stratified 5% sample (safe for all pandas versions)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1158/1158 [00:01<00:00, 618.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified sample created. Sample shape: (269176, 43)\n",
      "\n",
      "Feature engineering complete! Saved full dataset to: data/output/Traffic_PowerBI_Ready.csv\n",
      "Sample saved to: data/output/Traffic_PowerBI_Sample.csv\n"
     ]
    }
   ],
   "source": [
    "# Before running this script, perform exploratory data analysis to identify missing values\n",
    "# and any necessary type conversions or data cleansing.\n",
    "# The following code performs feature engineering to prepare the 'All Recording Traffic' dataset for Power BI:\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "tqdm.pandas(desc=\"Processing rows\")\n",
    "\n",
    "# 1. Load your traffic data from TXT\n",
    "df = pd.read_csv(\"EDA_Ready_All_Recording_Traffic.txt\", sep=\"\\t\")\n",
    "\n",
    "# 2. Convert DATE and DATETIME to datetime\n",
    "df[\"DATE\"] = pd.to_datetime(df[\"DATE\"])\n",
    "df[\"DATETIME\"] = pd.to_datetime(df[\"DATETIME\"])\n",
    "\n",
    "# 3. Extract Year and Month\n",
    "df[\"Year\"] = df[\"DATE\"].dt.year\n",
    "df[\"Month\"] = df[\"DATE\"].dt.month\n",
    "\n",
    "# 4. Is_Weekend flag\n",
    "df[\"Is_Weekend\"] = df[\"Day_Name\"].progress_apply(lambda d: 1 if d in [\"Saturday\",\"Sunday\"] else 0)\n",
    "\n",
    "# 5. Is_Holiday flag using pandas holiday calendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays(start=df[\"DATE\"].min(), end=df[\"DATE\"].max())\n",
    "df[\"Is_Holiday\"] = df[\"DATE\"].progress_apply(lambda d: 1 if d in holidays else 0)\n",
    "\n",
    "# 6. Season\n",
    "def get_season(m):\n",
    "    if m in [12,1,2]:\n",
    "        return \"Winter\"\n",
    "    elif m in [3,4,5]:\n",
    "        return \"Spring\"\n",
    "    elif m in [6,7,8]:\n",
    "        return \"Summer\"\n",
    "    else:\n",
    "        return \"Fall\"\n",
    "df[\"Season\"] = df[\"Month\"].progress_apply(get_season)\n",
    "\n",
    "# 7. Violation_Rate (vectorized)\n",
    "df[\"Violation_Rate\"] = (df[\"VIOLATION\"] / df[\"TOTAL\"]).fillna(0)\n",
    "\n",
    "# 8. Week_of_Year\n",
    "df[\"Week_of_Year\"] = df[\"DATE\"].dt.isocalendar().week\n",
    "\n",
    "# 9. Hour and Time_of_Day\n",
    "df[\"Hour\"] = df[\"DATETIME\"].dt.hour\n",
    "def tod(h):\n",
    "    if 6 <= h < 12:\n",
    "        return \"Morning\"\n",
    "    elif 12 <= h < 18:\n",
    "        return \"Afternoon\"\n",
    "    elif 18 <= h < 22:\n",
    "        return \"Evening\"\n",
    "    else:\n",
    "        return \"Night\"\n",
    "df[\"Time_of_Day\"] = df[\"Hour\"].progress_apply(tod)\n",
    "\n",
    "# 10. Lagged and rolling features\n",
    "df = df.sort_values([\"FAC_B\",\"DATETIME\"])\n",
    "df[\"Total_Lag1D\"] = df.groupby(\"FAC_B\")[\"TOTAL\"].shift(24)\n",
    "df[\"Total_Rolling_3D\"] = (\n",
    "    df.groupby(\"FAC_B\")[\"TOTAL\"]\n",
    "      .transform(lambda x: x.rolling(window=72, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "# 12. STRATIFIED SAMPLING (5%)\n",
    "df[\"month_year\"] = df[\"DATE\"].dt.to_period(\"M\")\n",
    "print(\"\\nCreating stratified 5% sample (safe for all pandas versions)...\")\n",
    "sample = (\n",
    "    df.groupby(['FAC_B', 'month_year'], group_keys=False)\n",
    "      .progress_apply(lambda x: x.sample(frac=0.05, random_state=1) if len(x) > 1 else x)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "print(f\"Stratified sample created. Sample shape: {sample.shape}\")\n",
    "\n",
    "\n",
    "# Save full dataset and sample\n",
    "output_file = \"data/output/Traffic_PowerBI_Ready.csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\nFeature engineering complete! Saved full dataset to: {output_file}\")\n",
    "\n",
    "sample_file = \"data/output/Traffic_PowerBI_Sample.csv\"\n",
    "sample.to_csv(sample_file, index=False)\n",
    "print(f\"Sample saved to: {sample_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b98281f-49dd-4d6a-8523-0296ca90e2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading PowerBI-ready file for monthly aggregation...\n",
      "Loaded 5383378 rows, 43 columns.\n",
      "Dropping unnecessary columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking columns to drop: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 67168.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating monthly values (this may take some time)...\n",
      "[1]\n",
      "\n",
      "Monthly aggregated data saved to: data/output/Traffic_PowerBI_Ready_Monthly.csv, shape: (1158, 22)\n"
     ]
    }
   ],
   "source": [
    "# (Optional) --- Monthly Aggregation Step ---\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "monthly_output_file = \"data/output/Traffic_PowerBI_Ready_Monthly.csv\"\n",
    "ready_file = \"data/output/Traffic_PowerBI_Ready.csv\"\n",
    "\n",
    "# Read with progress bar based on file size\n",
    "print(\"\\nLoading PowerBI-ready file for monthly aggregation...\")\n",
    "df_ready = pd.read_csv(ready_file)\n",
    "print(f\"Loaded {df_ready.shape[0]} rows, {df_ready.shape[1]} columns.\")\n",
    "\n",
    "# Remove unnecessary columns (with tqdm for column removal progress)\n",
    "remove_cols = [\n",
    "    'DAY', 'DATE', 'TIME', 'Day_Name', 'TIME_parsed', 'DATETIME', 'class_sum', 'class_mismatch',\n",
    "    'Year', 'Month', 'Is_Weekend', 'Is_Holiday', 'Violation_Rate', 'Week_of_Year',\n",
    "    'Hour', 'Time_of_Day', 'Total_Lag1D', 'Total_Rolling_3D'\n",
    "]\n",
    "print(\"Dropping unnecessary columns...\")\n",
    "cols_to_drop = [col for col in tqdm(remove_cols, desc=\"Checking columns to drop\") if col in df_ready.columns]\n",
    "df_monthly = df_ready.drop(columns=cols_to_drop)\n",
    "\n",
    "# tqdm for aggregation progress (simulate per group/row if desired)\n",
    "#group_keys = ['month_year', 'FAC', 'LANE', 'FAC_B']\n",
    "group_keys = ['month_year', 'FAC', 'FAC_B']\n",
    "value_cols = [col for col in df_monthly.columns if col not in group_keys]\n",
    "\n",
    "print(\"Aggregating monthly values (this may take some time)...\")\n",
    "df_monthly_agg = df_monthly.groupby(group_keys, group_keys=False)[value_cols].sum(numeric_only=True).reset_index()\n",
    "\n",
    "# Check how many rows per FAC per month\n",
    "print(df_monthly_agg.groupby(['month_year', 'FAC']).size().unique())\n",
    "\n",
    "# Save and confirm\n",
    "df_monthly_agg.to_csv(monthly_output_file, index=False)\n",
    "print(f\"\\nMonthly aggregated data saved to: {monthly_output_file}, shape: {df_monthly_agg.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca25f416-f749-40a8-bf17-6ef8d16d653c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding monthly lag and rolling average features...\n",
      "âœ… Enhanced monthly dataset with lag/rolling features saved to: data/output/Traffic_PowerBI_Ready_Monthly_Enhanced.csv\n"
     ]
    }
   ],
   "source": [
    "# ------------------ NEW SECTION: Monthly Lag and Rolling Features ------------------\n",
    "\n",
    "print(\"\\nAdding monthly lag and rolling average features...\")\n",
    "\n",
    "# Ensure datetime and proper sorting\n",
    "df_monthly_agg[\"month_year\"] = pd.to_datetime(df_monthly_agg[\"month_year\"])\n",
    "df_monthly_agg = df_monthly_agg.sort_values([\"FAC\", \"month_year\"])\n",
    "\n",
    "# Add lag features per facility\n",
    "df_monthly_agg[\"TOTAL_LAG_1M\"] = df_monthly_agg.groupby(\"FAC\")[\"TOTAL\"].shift(1)\n",
    "df_monthly_agg[\"TOTAL_LAG_3M\"] = df_monthly_agg.groupby(\"FAC\")[\"TOTAL\"].shift(3)\n",
    "\n",
    "# Add rolling mean feature (6-month moving average)\n",
    "df_monthly_agg[\"TOTAL_ROLL_6M\"] = (\n",
    "    df_monthly_agg.groupby(\"FAC\")[\"TOTAL\"]\n",
    "      .transform(lambda x: x.rolling(window=6, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "# Optionally fill NaNs from lags\n",
    "df_monthly_agg[[\"TOTAL_LAG_1M\", \"TOTAL_LAG_3M\"]] = df_monthly_agg[[\"TOTAL_LAG_1M\", \"TOTAL_LAG_3M\"]].bfill()\n",
    "\n",
    "\n",
    "# Save enhanced dataset\n",
    "enhanced_output_file = \"data/output/Traffic_PowerBI_Ready_Monthly_Enhanced.csv\"\n",
    "df_monthly_agg.to_csv(enhanced_output_file, index=False)\n",
    "print(f\"Enhanced monthly dataset with lag/rolling features saved to: {enhanced_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5227e101-80f1-4a8c-9cb2-a6a39aeac2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âž¡ï¸  Adding 6-month rolling total...\n",
      "âž¡ï¸  Calculating violation and payment ratios...\n",
      "âž¡ï¸  Adding yearly growth indicators...\n",
      "âœ… Enhanced dataset with new features saved to: data/output/Traffic_Clustering_Ready_Monthly.csv\n"
     ]
    }
   ],
   "source": [
    "# For Clustering Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()  # Enable progress bars\n",
    "\n",
    "# -------------------------------\n",
    "# Load your monthly aggregated dataset\n",
    "# -------------------------------\n",
    "df_monthly_agg = pd.read_csv(\"data/output/Traffic_PowerBI_Ready_Monthly.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# Ensure datetime and proper sorting\n",
    "# -------------------------------\n",
    "df_monthly_agg[\"month_year\"] = pd.to_datetime(df_monthly_agg[\"month_year\"])\n",
    "df_monthly_agg = df_monthly_agg.sort_values([\"FAC\", \"month_year\"])\n",
    "\n",
    "# -------------------------------\n",
    "# Add rolling mean feature (6-month moving average)\n",
    "# -------------------------------\n",
    "print(\"âž¡ï¸  Adding 6-month rolling total...\")\n",
    "df_monthly_agg[\"TOTAL_ROLL_6M\"] = (\n",
    "    df_monthly_agg.groupby(\"FAC\")[\"TOTAL\"]\n",
    "      .transform(lambda x: x.rolling(window=6, min_periods=1).mean())\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Add violation and payment ratios\n",
    "# -------------------------------\n",
    "print(\"âž¡ï¸  Calculating violation and payment ratios...\")\n",
    "df_monthly_agg[\"VIOLATION_RATIO\"] = df_monthly_agg[\"VIOLATION\"] / df_monthly_agg[\"TOTAL\"]\n",
    "df_monthly_agg[\"CASH_RATIO\"] = df_monthly_agg[\"CASH\"] / df_monthly_agg[\"TOTAL\"]\n",
    "df_monthly_agg[\"EZPASS_RATIO\"] = df_monthly_agg[\"EZPASS\"] / df_monthly_agg[\"TOTAL\"]\n",
    "df_monthly_agg[\"AUTOS_RATIO\"] = df_monthly_agg[\"Autos\"] / df_monthly_agg[\"TOTAL\"]\n",
    "df_monthly_agg[\"SMALL_T_RATIO\"] = df_monthly_agg[\"Small_T\"] / df_monthly_agg[\"TOTAL\"]\n",
    "df_monthly_agg[\"LARGE_T_RATIO\"] = df_monthly_agg[\"Large_T\"] / df_monthly_agg[\"TOTAL\"]\n",
    "df_monthly_agg[\"BUSES_RATIO\"] = df_monthly_agg[\"Buses\"] / df_monthly_agg[\"TOTAL\"]\n",
    "\n",
    "# -------------------------------\n",
    "# Add yearly growth indicators\n",
    "# -------------------------------\n",
    "print(\"âž¡ï¸  Adding yearly growth indicators...\")\n",
    "df_monthly_agg[\"YEAR\"] = df_monthly_agg[\"month_year\"].dt.year\n",
    "\n",
    "# Compute annual mean\n",
    "df_monthly_agg[\"ANNUAL_TOTAL_MEAN\"] = (\n",
    "    df_monthly_agg.groupby([\"FAC\", \"YEAR\"], group_keys=False)[\"TOTAL\"]\n",
    "    .transform(\"mean\")\n",
    ")\n",
    "\n",
    "# Compute year-over-year growth\n",
    "df_monthly_agg[\"YEARLY_GROWTH\"] = (\n",
    "    df_monthly_agg.groupby(\"FAC\", group_keys=False)[\"ANNUAL_TOTAL_MEAN\"]\n",
    "    .transform(lambda x: x.diff())\n",
    ")\n",
    "\n",
    "# Option 1: Fill NaN with 0\n",
    "df_monthly_agg[\"YEARLY_GROWTH\"] = df_monthly_agg[\"YEARLY_GROWTH\"].fillna(0)\n",
    "\n",
    "# -------------------------------\n",
    "# Save enhanced dataset (keep all original columns + new features)\n",
    "# -------------------------------\n",
    "enhanced_output_file = \"data/output/Traffic_Clustering_Ready_Monthly.csv\"\n",
    "df_monthly_agg.to_csv(enhanced_output_file, index=False)\n",
    "print(f\"âœ… Enhanced dataset with new features saved to: {enhanced_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bef607e-1b3b-48f7-86cb-7ab570e6321d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading PowerBI-ready file for weekly aggregation...\n",
      "Loaded 5383378 rows, 43 columns.\n",
      "Dropping unnecessary columns for weekly aggregation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking columns to drop: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00<00:00, 64411.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating weekly values (may take time)...\n",
      "\n",
      "Weekly aggregated data saved to: data/output/Traffic_PowerBI_Ready_Weekly.csv, shape: (37484, 23)\n"
     ]
    }
   ],
   "source": [
    "# (Optional) --- Weekly Aggregation Step ---\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "ready_file = \"data/output/Traffic_PowerBI_Ready.csv\"\n",
    "weekly_output_file = \"data/output/Traffic_PowerBI_Ready_Weekly.csv\"\n",
    "\n",
    "print(\"\\nLoading PowerBI-ready file for weekly aggregation...\")\n",
    "df_ready = pd.read_csv(ready_file)\n",
    "print(f\"Loaded {df_ready.shape[0]} rows, {df_ready.shape[1]} columns.\")\n",
    "\n",
    "remove_cols = [\n",
    "    'DAY', 'TIME', 'Day_Name', 'TIME_parsed', 'DATETIME', 'class_sum', 'class_mismatch',\n",
    "    'Year', 'Month', 'Is_Weekend', 'Is_Holiday', 'Violation_Rate', 'Week_of_Year',\n",
    "    'Hour', 'Time_of_Day', 'Total_Lag1D', 'Total_Rolling_3D'\n",
    "]\n",
    "print(\"Dropping unnecessary columns for weekly aggregation...\")\n",
    "cols_to_drop = [col for col in tqdm(remove_cols, desc=\"Checking columns to drop\") if col in df_ready.columns]\n",
    "df_weekly = df_ready.drop(columns=cols_to_drop)\n",
    "\n",
    "# Create a consistent week label if not present\n",
    "df_weekly['Week_of_Year'] = pd.to_datetime(df_weekly['DATE']).dt.isocalendar().week\n",
    "df_weekly['Year'] = pd.to_datetime(df_weekly['DATE']).dt.year\n",
    "\n",
    "# Optionally keep Is_Holiday, Is_Weekend, Day_Name, Season, etc. as \"first\"/\"any\" if useful for context\n",
    "group_keys = ['Year', 'Week_of_Year', 'FAC', 'LANE', 'FAC_B']\n",
    "value_cols = [col for col in df_weekly.columns if col not in group_keys]\n",
    "\n",
    "print(\"Aggregating weekly values (may take time)...\")\n",
    "df_weekly_agg = df_weekly.groupby(group_keys, group_keys=False)[value_cols].sum(numeric_only=True).reset_index()\n",
    "\n",
    "df_weekly_agg.to_csv(weekly_output_file, index=False)\n",
    "print(f\"\\nWeekly aggregated data saved to: {weekly_output_file}, shape: {df_weekly_agg.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e2246d-1bc0-445e-a19f-8daba6eff9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading PowerBI-ready file for daily aggregation...\n",
      "Loaded 5383378 rows, 43 columns.\n",
      "Dropping unnecessary columns for daily aggregation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking columns to drop: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 48358.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating daily values (may take time)...\n",
      "\n",
      "Daily aggregated data saved to: data/output/Traffic_PowerBI_Ready_Daily.csv, shape: (250044, 25)\n"
     ]
    }
   ],
   "source": [
    "# (Optional) --- Daily Aggregation Step ---\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "ready_file = \"data/output/Traffic_PowerBI_Ready.csv\"\n",
    "daily_output_file = \"data/output/Traffic_PowerBI_Ready_Daily.csv\"\n",
    "\n",
    "print(\"\\nLoading PowerBI-ready file for daily aggregation...\")\n",
    "df_ready = pd.read_csv(ready_file)\n",
    "print(f\"Loaded {df_ready.shape[0]} rows, {df_ready.shape[1]} columns.\")\n",
    "\n",
    "remove_cols = [\n",
    "    'DAY', 'TIME', 'TIME_parsed', 'DATETIME', 'class_sum', 'class_mismatch',\n",
    "    'Year', 'Month', 'Season', 'Violation_Rate', 'Week_of_Year',\n",
    "    'Hour', 'Time_of_Day', 'Total_Lag1D', 'Total_Rolling_3D'\n",
    "]\n",
    "print(\"Dropping unnecessary columns for daily aggregation...\")\n",
    "cols_to_drop = [col for col in tqdm(remove_cols, desc=\"Checking columns to drop\") if col in df_ready.columns]\n",
    "df_daily = df_ready.drop(columns=cols_to_drop)\n",
    "\n",
    "# For aggregation, keep DATE, FAC, LANE, FAC_B, Is_Holiday, Is_Weekend, Day_Name\n",
    "group_keys = ['DATE', 'FAC', 'LANE', 'FAC_B', 'Is_Holiday', 'Is_Weekend', 'Day_Name']\n",
    "value_cols = [col for col in df_daily.columns if col not in group_keys]\n",
    "\n",
    "print(\"Aggregating daily values (may take time)...\")\n",
    "df_daily_agg = df_daily.groupby(group_keys, group_keys=False)[value_cols].sum(numeric_only=True).reset_index()\n",
    "\n",
    "df_daily_agg.to_csv(daily_output_file, index=False)\n",
    "print(f\"\\nDaily aggregated data saved to: {daily_output_file}, shape: {df_daily_agg.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "580fe2e6-32d7-4255-84dc-615c1ce045f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Original dataset loaded.\n",
      "Shape: (250044, 26)\n",
      "Class distribution:\n",
      " VIOLATION_FLAG\n",
      "1    244744\n",
      "0      5300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ… Non-numeric columns converted (excluding DATE).\n",
      "\n",
      "Training set class distribution (before balancing):\n",
      "Counter({1: 195795, 0: 4240})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SMOTE+ENN in progress:   0%|                              | 0/1 [29:08<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Balancing complete!\n",
      "Training set class distribution (after balancing):\n",
      "Counter({0: 195738, 1: 194202})\n",
      "\n",
      "âœ… Files saved successfully.\n"
     ]
    }
   ],
   "source": [
    "#   This script prepares a balanced dataset to address the \n",
    "#   AutoML classification alert: \"Imbalanced classes were detected in your inputs.\"\n",
    "\n",
    "\n",
    "# Script: Balanced dataset for AutoML without altering DATE\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.combine import SMOTEENN\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# ===============================\n",
    "# Load dataset\n",
    "# ===============================\n",
    "df = pd.read_csv(\"data/output/Traffic_Classification_Ready_Daily.csv\")\n",
    "print(\"âœ… Original dataset loaded.\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Class distribution:\\n\", df['VIOLATION_FLAG'].value_counts())\n",
    "\n",
    "# ===============================\n",
    "# Encode all non-numeric columns except DATE\n",
    "# ===============================\n",
    "label_encoders = {}\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    if col != 'DATE':  # exclude DATE from encoding\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "print(\"\\nâœ… Non-numeric columns converted (excluding DATE).\")\n",
    "\n",
    "# ===============================\n",
    "# Split features & target\n",
    "# ===============================\n",
    "# Exclude DATE from SMOTE features\n",
    "features_to_balance = df.drop(columns=['VIOLATION_FLAG', 'DATE'])\n",
    "target = df['VIOLATION_FLAG']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_to_balance, target,\n",
    "    test_size=0.2,\n",
    "    stratify=target,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nTraining set class distribution (before balancing):\")\n",
    "print(Counter(y_train))\n",
    "\n",
    "# ===============================\n",
    "# Apply SMOTE+ENN only on numeric features\n",
    "# ===============================\n",
    "class TQDSMOTEENN(SMOTEENN):\n",
    "    def fit_resample(self, X, y):\n",
    "        for _ in tqdm(range(1), desc=\"SMOTE+ENN in progress\", ncols=80):\n",
    "            return super().fit_resample(X, y)\n",
    "\n",
    "sme = TQDSMOTEENN(random_state=42)\n",
    "X_train_bal, y_train_bal = sme.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nâœ… Balancing complete!\")\n",
    "print(\"Training set class distribution (after balancing):\")\n",
    "print(Counter(y_train_bal))\n",
    "\n",
    "# ===============================\n",
    "# Add back the DATE column to training & test sets\n",
    "# ===============================\n",
    "# For training set, match original indices where possible\n",
    "train_indices = X_train.index\n",
    "X_train_bal_df = pd.DataFrame(X_train_bal, columns=X_train.columns)\n",
    "\n",
    "# For simplicity, keep DATE as NaN for synthetic samples, or fill with a placeholder\n",
    "X_train_bal_df['DATE'] = pd.NA  \n",
    "\n",
    "# Test set keeps original DATE\n",
    "X_test_df = X_test.copy()\n",
    "X_test_df['DATE'] = df.loc[X_test.index, 'DATE'].values\n",
    "\n",
    "# Combine features + target\n",
    "balanced_train = pd.concat([X_train_bal_df,\n",
    "                            pd.Series(y_train_bal, name='VIOLATION_FLAG')], axis=1)\n",
    "balanced_test = pd.concat([X_test_df.reset_index(drop=True),\n",
    "                           y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# ===============================\n",
    "# Save balanced datasets\n",
    "# ===============================\n",
    "balanced_train.to_csv(\"data/output/Traffic_Classification_Balanced.csv\", index=False)\n",
    "balanced_test.to_csv(\"data/output/Traffic_Classification_Test.csv\", index=False)\n",
    "\n",
    "print(\"\\nâœ… Files saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f274b21e-aece-46d2-8c56-75f1dfc5d602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "# Note: This script calls / executes another Python script/notebook \n",
    "# ('data_ingestion_facility_speeds.ipynb' or its converted .py version) \n",
    "# to ensure the required data file 'Facility_Mobility_Speeds_Clean.csv' is generated beforehand.\n",
    "\n",
    "#import nbformat\n",
    "#from nbconvert.preprocessors import ExecutePreprocessor\n",
    "\n",
    "#def run_notebook(path):\n",
    "#    with open(path) as f:\n",
    "#        nb = nbformat.read(f, as_version=4)\n",
    "#    ep = ExecutePreprocessor(timeout=600, kernel_name='python3')\n",
    "#    ep.preprocess(nb, {'metadata': {'path': './'}})  # Change path if needed\n",
    "\n",
    "#run_notebook('data_ingestion_facility_speeds.ipynb')\n",
    "\n",
    "\n",
    "# Facility Mobility Speeds file cleaning. Removing extra columns\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Load the TXT file (tab-separated)\n",
    "df = pd.read_csv(\"Facility Mobility Speeds.txt\", sep=\"\\t\")\n",
    "\n",
    "# 2. Drop the static Facility_Order column\n",
    "df = df.drop(columns=[\"Facility_Order\"])\n",
    "\n",
    "# 3. Format Month_Year with a progress bar\n",
    "tqdm.pandas(desc=\"Formatting Month_Year\")\n",
    "df[\"Month_Year\"] = df[\"Month_Year\"].progress_apply(lambda x: pd.to_datetime(x).strftime(\"%Y-%m\"))\n",
    "\n",
    "# 4. Save the cleaned data for use in Power BI or further analysis\n",
    "output_file = \"Facility_Mobility_Speeds_Clean.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "# 5. Display a summary message\n",
    "print(f\"\\nFinal cleaned data ({df.shape[0]} records, {df.shape[1]} columns) saved to: {output_file}\")\n",
    "\n",
    "# Facility mobility speeds - Feature Engineering for Predictive Analysis (if required)\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_csv(\"Facility_Mobility_Speeds_Clean.csv\")\n",
    "\n",
    "# Feature engineering with progress bar\n",
    "tqdm.pandas(desc=\"Feature Engineering\")\n",
    "\n",
    "# 1. Extract year and month from Month_Year\n",
    "df[\"Month_Year\"] = pd.to_datetime(df[\"Month_Year\"], format=\"%Y-%m\")\n",
    "df[\"Year\"] = df[\"Month_Year\"].dt.year\n",
    "df[\"Month\"] = df[\"Month_Year\"].dt.month\n",
    "\n",
    "# 2. One-hot encode Direction\n",
    "df = pd.get_dummies(df, columns=[\"Direction\"], prefix=\"Dir\")\n",
    "\n",
    "# 3. (Optional) Create lag features for Avg_Speed (previous month per facility)\n",
    "df = df.sort_values([\"Facility\", \"Month_Year\"])\n",
    "df[\"Avg_Speed_Lag1\"] = df.groupby(\"Facility\")[\"Avg_Speed\"].shift(1)\n",
    "\n",
    "# 4. (Optional) Standardize continuous features\n",
    "#for col in [\"Freeflow\", \"Avg_Speed\", \"Delta\"]:\n",
    "#    mean = df[col].mean()\n",
    "#    std = df[col].std()\n",
    "#    df[f\"{col}_z\"] = (df[col] - mean) / std\n",
    "\n",
    "# 5. Save engineered features\n",
    "output_file = \"Facility_Mobility_Speeds_Features.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nFeature engineered data ({df.shape[0]} records, {df.shape[1]} columns) saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0cf6fe4-d554-4c36-a70e-49958ca77822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic_PowerBI_Ready.csv loaded successfully.\n",
      "Facility_Mobility_Speeds_Clean.csv loaded successfully.\n",
      "\n",
      "Processing Traffic Data...\n",
      "Traffic data aggregated and has 1158 monthly records.\n",
      "\n",
      "Processing Mobility Speed Data...\n",
      "Facility short codes replaced with full names.\n",
      "\n",
      "Merging DataFrames...\n",
      "\n",
      "--- Final Merged DataFrame Head ---\n",
      "  Facility_Name Month_Year  Aggregated_Traffic_Volume  Freeflow  Avg_Speed  \\\n",
      "0       Bayonne    2013-01                   279932.0       0.0        0.0   \n",
      "1       Bayonne    2013-02                   249747.0       0.0        0.0   \n",
      "2       Bayonne    2013-03                   283081.0       0.0        0.0   \n",
      "3       Bayonne    2013-04                   288071.0       0.0        0.0   \n",
      "4       Bayonne    2013-05                   302573.0       0.0        0.0   \n",
      "\n",
      "  Direction  Delta  \n",
      "0         0    0.0  \n",
      "1         0    0.0  \n",
      "2         0    0.0  \n",
      "3         0    0.0  \n",
      "4         0    0.0  \n",
      "\n",
      "Shape of the final merged DataFrame: (1334, 7)\n",
      "\n",
      "Data successfully saved to Merged_Traffic_Mobility_Data_Full.csv\n",
      "\n",
      "Filtering data to keep only rows where Aggregated_Traffic_Volume and Freeflow are non-zero...\n",
      "\n",
      "--- Saving Filtered Merged DataFrame ---\n",
      "    Facility_Name Month_Year  Aggregated_Traffic_Volume  Freeflow  Avg_Speed  \\\n",
      "313     GWB Lower    2024-01                  1685357.0     45.90       34.5   \n",
      "314     GWB Lower    2024-01                  1685357.0     46.05       25.6   \n",
      "315     GWB Lower    2024-02                  1673098.0     45.90       32.2   \n",
      "316     GWB Lower    2024-02                  1673098.0     46.10       22.5   \n",
      "317     GWB Lower    2024-03                  1877547.0     45.90       32.4   \n",
      "\n",
      "    Direction  Delta  \n",
      "313        WB  -11.4  \n",
      "314        EB  -20.5  \n",
      "315        WB  -13.7  \n",
      "316        EB  -23.6  \n",
      "317        WB  -13.5  \n",
      "Shape of the final merged and filtered DataFrame: (32, 7)\n",
      "Data successfully saved to Merged_Traffic_Mobility_Data_FILTERED.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Pipeline: Traffic and Mobility Data Merge and Aggregation\n",
    "\n",
    "PURPOSE:\n",
    "Aggregates hourly traffic data ('Traffic_PowerBI_Ready.csv') to a monthly level,\n",
    "harmonizes facility names with mobility speed data ('Facility_Mobility_Speeds_Clean.csv'), \n",
    "performs an outer merge, and saves two distinct output files.\n",
    "\n",
    "INPUTS:\n",
    "1. 'Traffic_PowerBI_Ready.csv': Hourly traffic counts, requires aggregation by Month/Facility.\n",
    "2. 'Facility_Mobility_Speeds_Clean.csv': Already aggregated monthly speed data using short facility codes.\n",
    "    ** Note: To generate this file, run data_ingestion_facility_speeds.ipynb beforehand.\n",
    "\n",
    "OUTPUTS:\n",
    "1. 'Merged_Traffic_Mobility_Data_FULL.csv': \n",
    "    - Contains all combinations of Facility and Month from both input files (Outer Join).\n",
    "    - Missing metric values (where data existed in one file but not the other) are filled with 0.\n",
    "2. 'Merged_Traffic_Mobility_Data_FILTERED.csv':\n",
    "    - Filtered version of the FULL dataset.\n",
    "    - Only includes rows where both 'Aggregated_Traffic_Volume' (from Traffic) AND 'Freeflow' \n",
    "      (the primary speed metric) are non-zero. Use this file for direct comparison analysis.\n",
    "\n",
    "ASSUMPTIONS & NOTES:\n",
    "- The 'Traffic_PowerBI_Ready.csv' file uses the column 'TOTAL' for traffic volume.\n",
    "- The traffic data contains historical records (e.g., 2013-2025), while the speed data \n",
    "  is currently limited (e.g., 2024-2025). The FULL merge preserves all historical traffic \n",
    "  data alongside available speed data.\n",
    "- Facility names are mapped using the SHORT_TO_LONG_MAPPING defined below.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Define Mapping ---\n",
    "# Mapping from Facility short code (in Facility_Mobility_Speeds_Clean) \n",
    "# to full name (in Traffic_PowerBI_Ready)\n",
    "SHORT_TO_LONG_MAPPING = {\n",
    "    \"BB\": \"Bayonne Bridge\",\n",
    "    \"GB\": \"Goethals Bridge\",\n",
    "    # The GWB short code maps to 'GWB Lower' for consistency when merging with the \n",
    "    # granular traffic data which may contain 'GWB Lower' and 'GWB PIP'.\n",
    "    \"GWB\": \"GWB Lower\", \n",
    "    \"HT\": \"Holland Tunnel\",\n",
    "    \"LT\": \"Lincoln Tunnel\",\n",
    "    \"OBX\": \"Outerbridge Crossing\"\n",
    "}\n",
    "\n",
    "# --- 2. Load Data ---\n",
    "\n",
    "# Load Traffic_PowerBI_Ready.csv (hourly data with FAC_B and 'TOTAL' column)\n",
    "df_traffic = pd.read_csv('data/output/Traffic_PowerBI_Ready.csv') \n",
    "print(\"Traffic_PowerBI_Ready.csv loaded successfully.\")\n",
    "\n",
    "# DEBUGGING STEP: Removed after identifying the issue (TOTAL vs Total)\n",
    "\n",
    "# Load Facility_Mobility_Speeds_Clean.csv (aggregated speed data with 'Facility' short code)\n",
    "df_speeds = pd.read_csv(\"Facility_Mobility_Speeds_Clean.csv\")\n",
    "print(\"Facility_Mobility_Speeds_Clean.csv loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- 3. Process Traffic_PowerBI_Ready (Aggregation) ---\n",
    "print(\"\\nProcessing Traffic Data...\")\n",
    "\n",
    "# Convert the time column ('month_year') to a monthly period for aggregation\n",
    "df_traffic['Month_Year'] = pd.to_datetime(df_traffic['month_year']).dt.to_period('M')\n",
    "\n",
    "# Aggregate the hourly data by FAC_B and Month_Year, summing the CORRECT 'TOTAL' column.\n",
    "df_traffic_agg = df_traffic.groupby(['FAC_B', 'Month_Year']).agg(\n",
    "    Aggregated_Traffic_Volume=('TOTAL', 'sum')  # <-- CORRECTED: Changed 'Total' to 'TOTAL'\n",
    ").reset_index()\n",
    "\n",
    "# Rename the facility column to the common merging key\n",
    "df_traffic_agg.rename(columns={'FAC_B': 'Facility_Name'}, inplace=True)\n",
    "print(f\"Traffic data aggregated and has {len(df_traffic_agg)} monthly records.\")\n",
    "\n",
    "\n",
    "# --- 4. Process Facility_Mobility_Speeds_Clean (Mapping) ---\n",
    "print(\"\\nProcessing Mobility Speed Data...\")\n",
    "\n",
    "# Convert the time column ('Month_Year') to a monthly period for merging\n",
    "df_speeds['Month_Year'] = pd.to_datetime(df_speeds['Month_Year']).dt.to_period('M')\n",
    "\n",
    "# Map the short Facility codes (BB, GB) to the full names \n",
    "df_speeds['Facility_Name'] = df_speeds['Facility'].replace(SHORT_TO_LONG_MAPPING)\n",
    "\n",
    "# Drop the old short code column\n",
    "df_speeds.drop(columns=['Facility'], inplace=True)\n",
    "print(\"Facility short codes replaced with full names.\")\n",
    "\n",
    "\n",
    "# --- 5. Merge Data and Fill NaN ---\n",
    "print(\"\\nMerging DataFrames...\")\n",
    "\n",
    "# Perform an 'outer' merge on the common keys ['Facility_Name', 'Month_Year']\n",
    "merged_df = pd.merge(\n",
    "    df_traffic_agg, \n",
    "    df_speeds, \n",
    "    on=['Facility_Name', 'Month_Year'], \n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Identify the columns that need to be zero-filled (the numeric metric columns)\n",
    "key_columns = ['Facility_Name', 'Month_Year']\n",
    "metric_columns = [col for col in merged_df.columns if col not in key_columns]\n",
    "\n",
    "# Fill non-matching metric values (NaNs from the outer join) with 0\n",
    "merged_df[metric_columns] = merged_df[metric_columns].fillna(0)\n",
    "\n",
    "# Convert Month_Year back to a standard string format for final output\n",
    "merged_df['Month_Year'] = merged_df['Month_Year'].astype(str)\n",
    "\n",
    "print(\"\\n--- Final Merged DataFrame Head ---\")\n",
    "print(merged_df.head())\n",
    "print(f\"\\nShape of the final merged DataFrame: {merged_df.shape}\")\n",
    "\n",
    "# --- 6. Save the Merged Data ---\n",
    "merged_df.to_csv('data/output/Merged_Traffic_Mobility_Data_Full.csv', index=False)\n",
    "print(\"\\nData successfully saved to Merged_Traffic_Mobility_Data_Full.csv\")\n",
    "\n",
    "\n",
    "# --- 6. Filter for Non-Zero Metrics (Create the FILTERED Dataframe) ---\n",
    "# Filter to keep only rows where the main traffic metric AND \n",
    "# the primary speed metric (Freeflow) are non-zero.\n",
    "# We create a new DataFrame 'filtered_df' for this specific use case.\n",
    "\n",
    "filtered_df = merged_df.copy()\n",
    "\n",
    "if 'Freeflow' in filtered_df.columns:\n",
    "    print(\"\\nFiltering data to keep only rows where Aggregated_Traffic_Volume and Freeflow are non-zero...\")\n",
    "    filtered_df = filtered_df[\n",
    "        (filtered_df['Aggregated_Traffic_Volume'] != 0) & \n",
    "        (filtered_df['Freeflow'] != 0)\n",
    "    ].copy()\n",
    "else:\n",
    "    print(\"\\nWarning: 'Freeflow' column not found for non-zero filtering. Skipping non-zero filter for filtered file.\")\n",
    "\n",
    "# Convert Month_Year back to a standard string format for final output (FILTERED data)\n",
    "filtered_df['Month_Year'] = filtered_df['Month_Year'].astype(str)\n",
    "\n",
    "print(\"\\n--- Saving Filtered Merged DataFrame ---\")\n",
    "print(filtered_df.head())\n",
    "print(f\"Shape of the final merged and filtered DataFrame: {filtered_df.shape}\")\n",
    "\n",
    "# --- 7. Save the Filtered Data ---\n",
    "filtered_df.to_csv('data/output/Merged_Traffic_Mobility_Data_FILTERED.csv', index=False)\n",
    "print(\"Data successfully saved to Merged_Traffic_Mobility_Data_FILTERED.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c97d33c-449a-435e-9fab-d0473cc0dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective: Generate a dataset with traffic volume categories (High / Medium / Low)\n",
    "# for each facility, so it can be used to train a classification model in AutoML\n",
    "# to predict future traffic volume categories based on daily features.\n",
    "\n",
    "# To achieve this, we:\n",
    "# 1. Aggregate lane-level daily traffic data to facility-level totals.\n",
    "# 2. Apply clustering on total traffic for each facility to assign High / Medium / Low labels.\n",
    "# 3. Visualize clusters to understand traffic patterns and support future predictive modeling.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------- LOAD DATA ----------\n",
    "df = pd.read_csv(\"data/output/Traffic_Classification_Ready_Daily.csv\", parse_dates=['DATE'])\n",
    "\n",
    "# ------------------- AGGREGATE DAILY DATA PER FACILITY -------------------\n",
    "agg_cols = ['TOTAL', 'Autos', 'Small_T', 'Large_T', 'Buses', 'VIOLATION']\n",
    "daily_fac_df = df.groupby(['DATE','FAC']).agg({col:'sum' for col in agg_cols}).reset_index()\n",
    "\n",
    "print(\"Unique facilities:\", sorted(daily_fac_df['FAC'].unique()))\n",
    "print(daily_fac_df['FAC'].value_counts())\n",
    "\n",
    "# Keep relevant features for clustering\n",
    "cluster_features = ['TOTAL']  # you can add other numeric columns if needed\n",
    "\n",
    "# ------------------- CLUSTERING PER FACILITY -------------------\n",
    "volume_levels = []\n",
    "facilities = sorted(daily_fac_df['FAC'].unique())  # only existing FAC values\n",
    "\n",
    "for fac in facilities:\n",
    "    fac_df = daily_fac_df[daily_fac_df['FAC'] == fac].copy()\n",
    "    \n",
    "    # Using 3 clusters for High, Medium, Low\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "    fac_df['Cluster'] = kmeans.fit_predict(fac_df[cluster_features])\n",
    "    \n",
    "    # Map clusters to High/Medium/Low based on mean TOTAL per cluster\n",
    "    cluster_means = fac_df.groupby('Cluster')['TOTAL'].mean().sort_values(ascending=False)\n",
    "    mapping = {cluster: level for cluster, level in zip(cluster_means.index, ['High','Medium','Low'])}\n",
    "    fac_df['Volume_Level'] = fac_df['Cluster'].map(mapping)\n",
    "    \n",
    "    volume_levels.append(fac_df)\n",
    "\n",
    "# Combine all facilities\n",
    "final_df = pd.concat(volume_levels).sort_values(['FAC','DATE'])\n",
    "\n",
    "# ------------------- VISUALIZE -------------------\n",
    "# Separate figure per facility to avoid clutter\n",
    "for fac in facilities:\n",
    "    fac_df = final_df[final_df['FAC'] == fac]\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.scatterplot(\n",
    "        data=fac_df,\n",
    "        x='DATE',\n",
    "        y='TOTAL',\n",
    "        hue='Volume_Level',\n",
    "        palette='viridis'\n",
    "    )\n",
    "    plt.title(f'FAC {fac} Traffic Clusters Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Total Traffic')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Volume Level')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---------- SAVE DATASETS ----------\n",
    "# 1. For visualization (keeps internal Cluster info)\n",
    "final_df.to_csv(\"data/output/Traffic_Facility_Clustered_With_Cluster.csv\", index=False)\n",
    "\n",
    "# 2. For AutoML (drops Cluster to avoid target leakage)\n",
    "automl_df = final_df.drop(columns=['Cluster'])\n",
    "automl_df.to_csv(\"data/output/Traffic_Facility_Clustered_For_Classification.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
